eksctl get cluster --region=us-east-1
eksctl create cluster -f eksctl-ipv4-cluster.yml &
eksctl get nodegroups  --cluster=eks-cluster-1 --region=us-east-1
eksctl delete nodegroup nodegroup1 --cluster=eks-cluster-1 --region=us-east-1

eksctl delete cluster --region=us-east-1 --name=eks-cluster-1 &
Go to Cloudformation and delete if any pending stacks exists.
aws sts get-caller-identity
aws eks update-kubeconfig --region us-east-1 --name eks-cluster-1
kubectl get nodes
kubectl get pods -A
ku delete -f Ingress.yml -f deployment.yml


#Make sure you have deleted old cloudformation stacks before deploying below commands.
#The below cluster needs 25 mins cor creation.
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: eks-cluster-1
  region: us-east-1
  version: "1.30"

kubernetesNetworkConfig:
  ipFamily: IPv4

addons:
  - name: vpc-cni
    version: latest
  - name: coredns
    version: latest
  - name: kube-proxy
    version: latest
  - name: aws-ebs-csi-driver
    version: latest
  - name: eks-pod-identity-agent
    version: latest

iam:
  withOIDC: true

managedNodeGroups:
  - name: nodegroup1
    instanceType: t2.large
    minSize: 2
    desiredCapacity: 2
    maxSize: 4
    availabilityZones: ["us-east-1a", "us-east-1b"]
    volumeSize: 50
    ssh:
      allow: true
      publicKeyPath: ~/.ssh/id_rsa.pub


#Adding eks-pod-identity-agent addon
#https://repost.aws/articles/AR5ogohRSfRzCFh8ooFeq8kg/how-to-enable-amazon-eks-pod-identity-and-assign-role-to-service-account-running-workloads

aws eks create-addon \
--cluster-name REPLACEME_WITH_CLUSTER_NAME \
--addon-name eks-pod-identity-agent \
--addon-version v1.3.2-eksbuild.2

root@ip-10-40-1-98:~# ku get pods -A | grep -i eks-pod-identity
kube-system   eks-pod-identity-agent-59vw6                    1/1     Running   0          86s
kube-system   eks-pod-identity-agent-7rs6j                    1/1     Running   0          86s
kube-system   eks-pod-identity-agent-nrbb7                    1/1     Running   0  

Make sure you have updated the Role Trust Policy & IAM Policy pod-idenity.json

aws eks create-pod-identity-association \
--cluster-name eks-cluster-1 \
--namespace default --service-account default \
--role-arn arn:aws:iam::721834156908:role/awsb63-ec2-sessions-manager-role \
--region us-east-1


#EKS Cluster Upgrade.
eksctl upgrade cluster --name=eks-cluster-1 --version=1.31 --region=us-east-1 --approve
eksctl get cluster --region=us-east-1

eksctl upgrade nodegroup1 \
  --name=node-group-name \
  --cluster=eks-cluster-1 \
  --region=us-east-1 \
  --kubernetes-version=1.31

eksctl get nodegroups  --cluster=eks-cluster-1 --region=us-east-1

You cannot downgrade the Kubernetes of an Amazon EKS cluster. Instead, create a new cluster on a previous Amazon EKS version and migrate the workloads.

#       cd
mv kubectl kubectl_1.24
mv bin bin_1.24 
curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.15/2023-01-11/bin/linux/amd64/kubectl
chmod +x ./kubectl
mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin
# kubectl version

## To view workloads of cluster in eks UI, give arn in aws auth config:
# kubectl edit configmap -n kube-system aws-auth
# mapUsers: |
#     - userarn: arn:aws:iam::051826694723:root
#       username: admin
#       groups:
#         - system:masters






